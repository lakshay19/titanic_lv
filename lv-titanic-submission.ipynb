{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code structure Roadmap**\n\n1. EDA\n   * Identifying most important features based on their relationship with the survived column\n        * Visualizations       \n   * Which variables have null values?\n3. Feature engineering\n   * Apply all changes to both train and test data\n   * Replace/drop null values (Imputation)\n   * Outliers\n   * Create new features\n        * Binning\n        * Extract features\n   * Convert string categoricals\n4. Model Selection\n   * For each selected model:\n        * Hyperparameter tuning\n        * Cross-validation\n5. Final model\n   * Apply selected model to whole training data\n   * Final results submit\n","metadata":{}},{"cell_type":"code","source":"# Import visualizations libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import training and test data\ntitanic_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntitanic_test = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking head\ntitanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(titanic_train.isnull(),yticklabels=False,cmap='viridis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.set_palette('RdBu_r')\nsns.countplot(data=titanic_train,x='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Sex',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = list(titanic_train[titanic_train['Survived'] == 1]['Age'])\nx2 = list(titanic_train[titanic_train['Survived'] == 0]['Age'])\ncolors = ['#E69F00', '#56B4E9']\nnames = ['Survived', 'Did not survive']\nplt.hist([x1, x2], stacked=True,color = colors, label=names,bins=30)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Pclass',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = list(titanic_train[titanic_train['Pclass'] == 1]['Age'])\nx2 = list(titanic_train[titanic_train['Pclass'] == 2]['Age'])\nx3 = list(titanic_train[titanic_train['Pclass'] == 3]['Age'])\ncolors = ['#E69F00', '#56B4E9', '#009E73']\nnames = ['First Class', 'Second Class', ' Third Class']\nplt.figure(figsize=(15,5))\nplt.hist([x1, x2, x3], stacked=True,color = colors, label=names,bins=30)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train[['Pclass', 'Age']].groupby(['Pclass'], as_index=False).median().sort_values(by='Pclass', ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = list(titanic_train[titanic_train['Survived'] == 1]['Fare'])\nx2 = list(titanic_train[titanic_train['Survived'] == 0]['Fare'])\ncolors = ['#E69F00', '#56B4E9']\nnames = ['Survived', 'Did not survive']\nplt.figure(figsize=(10,5))\nplt.hist([x1, x2], stacked=True,color = colors, label=names,bins=30)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = list(titanic_train[titanic_train['Pclass'] == 1]['Fare'])\nx2 = list(titanic_train[titanic_train['Pclass'] == 2]['Fare'])\nx3 = list(titanic_train[titanic_train['Pclass'] == 3]['Fare'])\ncolors = ['#E69F00', '#56B4E9', '#009E73']\nnames = ['First Class', 'Second Class', ' Third Class']\nplt.figure(figsize=(15,5))\nplt.hist([x1, x2, x3], stacked=True,color = colors, label=names,bins=50)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='SibSp',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Parch',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Embarked',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Embarked',hue='Pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Embarked',hue='Sex')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Key observations from EDA:\n* Should bin 'Age' and 'Fare'. Both of them seem to have cases which might lead to overfitting.\n* Consider dropping 'Embarked' column. Not sure if it would add any value.\n* Drop 'PassengerId'\n* Should drop 'Cabin'. Too many null values\n* Would need to fill NULL values in 'Age'\n* Only two NULL values in 'Embarked'. Fill or just drop those two?","metadata":{}},{"cell_type":"code","source":"titanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping columns not required\ncombine = [titanic_train, titanic_test]\n\nprint(\"Before\", titanic_train.shape, titanic_test.shape, combine[0].shape, combine[1].shape)\n\ntitanic_train = titanic_train.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ntitanic_test = titanic_test.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ncombine = [titanic_train, titanic_test]\n\nprint(\"After\", titanic_train.shape, titanic_test.shape, combine[0].shape, combine[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \ntitanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(titanic_train['Title'], titanic_train['Sex'],margins=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(titanic_train['Title'], titanic_train['Survived'],margins=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(titanic_test['Title'], titanic_train['Survived'],margins=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \nsns.countplot(data=titanic_train,x='Title',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Title', ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_test,x='Title')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_port = titanic_train.Embarked.dropna().mode()[0]\nfreq_port","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing NULL values in 'Embarked' with the most probable port\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n\ntitanic_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dummies for categorical columns which are not ordinal\n\n# Training data\nsex = pd.get_dummies(titanic_train['Sex'],drop_first=True)\ntitle = pd.get_dummies(titanic_train['Title'],drop_first=True)\nembark = pd.get_dummies(titanic_train['Embarked'],drop_first=True)\ntitanic_train = pd.concat([titanic_train,sex,title,embark],axis=1)\n\ntitanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data\nsex = pd.get_dummies(titanic_test['Sex'],drop_first=True)\ntitle = pd.get_dummies(titanic_test['Title'],drop_first=True)\nembark = pd.get_dummies(titanic_test['Embarked'],drop_first=True)\ntitanic_test = pd.concat([titanic_test,sex,title,embark],axis=1)\n\ntitanic_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping columns not needed anymore\ntitanic_train = titanic_train.drop(['Name', 'Sex', 'Embarked','Title'], axis=1)\ntitanic_test = titanic_test.drop(['Name', 'Sex', 'Embarked','Title'], axis=1)\ncombine = [titanic_train, titanic_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating new family column\nfor dataset in combine:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntitanic_train[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=titanic_train,x='Family',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging anything more than 8 into 8\nfor dataset in combine:    \n    dataset.loc[ dataset['Family'] >= 8, 'Family'] = 8\n    \nsns.countplot(data=titanic_train,x='Family',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train[['Pclass', 'Fare']].groupby(['Pclass'], as_index=False).median().sort_values(by='Pclass', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling NULL Values in Fare\ndef impute_Fare(cols):\n    Fare = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Fare):\n\n        if Pclass == 1:\n            return 60.2875\n\n        elif Pclass == 2:\n            return 14.2500\n\n        else:\n            return 8.0500\n\n    else:\n        return Fare\n\ntitanic_train['Fare'] = titanic_train[['Fare','Pclass']].apply(impute_Fare,axis=1)\ntitanic_test['Fare'] = titanic_test[['Fare','Pclass']].apply(impute_Fare,axis=1)\n\ntitanic_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling NULL Ages using median age based on sex and Pclass\n\nguess_ages = np.zeros((2,3))\n\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = titanic_train[(titanic_train['male'] == i) & (titanic_train['Pclass'] == j+1)]['Age'].dropna()\n\n        age_guess = guess_df.median()\n\n        guess_ages[i,j] = int(age_guess)\n            \nguess_ages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine = [titanic_train, titanic_test]\nfor dataset in combine:          \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.male == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntitanic_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Age bands\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n\nsns.countplot(data=titanic_train,x='Age',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train['FareBand'] = pd.qcut(titanic_train['Fare'], 4)\ntitanic_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train = titanic_train.drop(['FareBand'], axis=1)\ncombine = [titanic_train, titanic_test]\n\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \nsns.countplot(data=titanic_train,x='Fare',hue='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping columns not needed anymore\ntitanic_train = titanic_train.drop(['SibSp', 'Parch'], axis=1)\ntitanic_test = titanic_test.drop(['SibSp', 'Parch'], axis=1)\n\nprint(titanic_train.shape, titanic_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models to try:\n* Logistic regression\n* SVM\n* PCA + SVM (There are four additional columns due to dummies)\n* Random forrest","metadata":{}},{"cell_type":"code","source":"# Creating X and y\nX = titanic_train.drop('Survived',axis=1)\ny = titanic_train['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating PCA components to try alternatively\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.decomposition import PCA\npca_st = PCA(n_components = 0.95)\npca_st.fit(X_scaled)\nX_pca = pca_st.transform(X_scaled)\n\nX_pca.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(logr, X, y, cv=5)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression with PCA\nlogr_pca = LogisticRegression()\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(logr_pca, X_pca, y, cv=5)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Logistic Regression with hyperparameter tuning\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import linear_model,decomposition\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\n# Create an scaler object\nsc = StandardScaler()\n\n# Create a pca object\npca = decomposition.PCA()\n\n# Create a logistic regression object with an L2 penalty\nlogistic = linear_model.LogisticRegression()\n\n# Create a pipeline of three steps. First, standardize the data.\n# Second, tranform the data with PCA.\n# Third, train a logistic regression on the data.\npipe = Pipeline(steps=[('sc', sc),\n                       ('pca', pca),\n                       ('logistic', logistic)])\n\n# Create Parameter Space\n# Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1)\nn_components = list(range(1,X.shape[1]+1,1))\n# Create a list of values of the regularization parameter\nC = np.logspace(-4, 4, 50)\n# Create a list of options for the regularization penalty\npenalty = ['l1', 'l2']\n# Create a dictionary of all the parameter options \n# Note has you can access the parameters of steps of a pipeline by using '__â€™\nparameters = dict(pca__n_components=n_components,\n                  logistic__C=C,\n                  logistic__penalty=penalty)\n\n# Conduct Parameter Optmization With Pipeline\n# Create a grid search object\nclf = GridSearchCV(pipe, parameters)\n\n# Fit the grid search\nclf.fit(X, y)\n# View The Best Parameters\nprint('Best Penalty:', clf.best_estimator_.get_params()['logistic__penalty'])\nprint('Best C:', clf.best_estimator_.get_params()['logistic__C'])\nprint('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\nprint(); print(clf.best_estimator_.get_params()['logistic'])\n\n# Use Cross Validation To Evaluate Model\nCV_Result = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\nprint(); print(CV_Result)\nprint(); print(CV_Result.mean())\nprint(); print(CV_Result.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression results:\n* The normal logistic regression works as good as the tuned model\n* PCA doesn't seem to provide good results since the best parameter included all the variables","metadata":{}},{"cell_type":"code","source":"# SVM\nfrom sklearn.svm import SVC\n\nsup_vec = SVC()\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(sup_vec, X, y, cv=5)\nprint(scores.mean())\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM with PCA\nfrom sklearn.svm import SVC\n\nsup_vec_pca = SVC()\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(sup_vec_pca, X_pca, y, cv=5)\nprint(scores.mean())\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM with Hyperparameter tuning\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n\nfrom sklearn.model_selection import GridSearchCV\n\nsup_vec_ht = GridSearchCV(SVC(),param_grid,refit=True,verbose=3, n_jobs = -1)\nsup_vec_ht.fit(X, y)\n\nsup_vec_ht.best_params_\nsup_vec_ht.best_estimator_\n\n# Use Cross Validation To Evaluate Model\nCV_svm_Result = cross_val_score(sup_vec_ht, X, y, cv=5, n_jobs=-1)\nprint(); print(CV_svm_Result)\nprint(); print(CV_svm_Result.mean())\nprint(); print(CV_svm_Result.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM Results: Usual SVM works better compared to SVM with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# Random forrest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rfc, X, y, cv=5)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random forrest with PCA\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_pca = RandomForestClassifier(n_estimators=100)\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rfc_pca, X_pca, y, cv=5)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forrest with Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 6, 8, 12],\n               'min_samples_split': [5, 7, 10, 14],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n\nrfc_rs = RandomForestClassifier()\nrfc_rs_ht = RandomizedSearchCV(estimator = rfc_rs, param_distributions = random_search, n_iter = 80, \n                                  cv = 4, verbose= 3, random_state= 101, n_jobs = -1)\nrfc_rs_ht.fit(X, y)\n\nrfc_rs_ht.best_params_\nrfc_rs_ht.best_estimator_\n\nCV_rfc_rs_ht = cross_val_score(rfc_rs_ht, X, y, cv=5, n_jobs=-1)\nprint(); print(CV_rfc_rs_ht)\nprint(); print(CV_rfc_rs_ht.mean())\nprint(); print(CV_rfc_rs_ht.std())\n\ngrid_search = {\n    'criterion': [rfc_rs_ht.best_params_['criterion']],\n    'max_depth': [rfc_rs_ht.best_params_['max_depth']],\n    'max_features': [rfc_rs_ht.best_params_['max_features']],\n    'min_samples_leaf': [rfc_rs_ht.best_params_['min_samples_leaf'] - 2, \n                         rfc_rs_ht.best_params_['min_samples_leaf'], \n                         rfc_rs_ht.best_params_['min_samples_leaf'] + 2],\n    'min_samples_split': [rfc_rs_ht.best_params_['min_samples_split'] - 3, \n                          rfc_rs_ht.best_params_['min_samples_split'], \n                          rfc_rs_ht.best_params_['min_samples_split'] + 3],\n    'n_estimators': [rfc_rs_ht.best_params_['n_estimators'] - 150, \n                     rfc_rs_ht.best_params_['n_estimators'] - 100, \n                     rfc_rs_ht.best_params_['n_estimators'], \n                     rfc_rs_ht.best_params_['n_estimators'] + 100, \n                     rfc_rs_ht.best_params_['n_estimators'] + 150]\n}\n\nrfc_gs = RandomForestClassifier()\nrfc_gs_ht = GridSearchCV(estimator = rfc_gs, param_grid = grid_search, cv = 4, verbose= 3, n_jobs = -1)\nrfc_gs_ht.fit(X, y)\n\nrfc_gs_ht.best_params_\nrfc_gs_ht.best_estimator_\n\nfrom sklearn.model_selection import cross_val_score\n\n# Use Cross Validation To Evaluate Model\nCV_rfc_gs_ht = cross_val_score(rfc_gs_ht, X, y, cv=5, n_jobs=-1)\nprint(); print(CV_rfc_gs_ht)\nprint(); print(CV_rfc_gs_ht.mean())\nprint(); print(CV_rfc_gs_ht.std())","metadata":{"trusted":true}},{"cell_type":"markdown","source":"Random forrest with hyperparameter tuning works best","metadata":{}},{"cell_type":"code","source":"# predicting using the final selected model\nsup_vec.fit(X, y)\n\npredictions = sup_vec.predict(titanic_test)\n\n# importing again to get the passengerid column\ntest_import = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# creating submission file\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_import[\"PassengerId\"],\n        \"Survived\": predictions\n    })\n\nsubmission.to_csv('lv_submission_svm.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}